{
	"jobConfig": {
		"name": "bronze-nyc-taxi-etl-dev",
		"description": "",
		"role": "arn:aws:iam::324728439677:role/AWSGlueServiceRole-DataPlatform",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 2,
		"security": "none",
		"scriptName": "bronze-nyc-taxi-etl-dev.py",
		"scriptLocation": "s3://teedot-data-lake-dev/aws/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-12-09T12:27:36.380Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://teedot-data-lake-dev",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-enable",
		"sparkPath": "s3://teedot-data-lake-dev/aws/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nimport json\nfrom datetime import datetime\n\nimport boto3\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.utils import getResolvedOptions\n\n# ---------------------------------------------------------------\n# Job Arguments\n# ---------------------------------------------------------------\nargs = getResolvedOptions(\n    sys.argv,\n    [\"JOB_NAME\", \"run_id\", \"input_uri\"]\n)\n\ninput_uri = args[\"input_uri\"]\nrun_id = args[\"run_id\"]\n\n# ---------------------------------------------------------------\n# Spark & Glue Context Setup\n# ---------------------------------------------------------------\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\njob = Job(glueContext)\njob.init(args[\"JOB_NAME\"], args)\n\n# ---------------------------------------------------------------\n# Read Raw Input\n# ---------------------------------------------------------------\nraw_df = spark.read.parquet(input_uri)\n\n# ---------------------------------------------------------------\n# Data Normalization Logic\n# ---------------------------------------------------------------\ndef normalize_data(df):\n\n    start_date = F.to_date(F.lit(\"2024-01-01\"))\n\n    return (\n        df\n        .filter(F.col(\"total_amount\") > 0)\n\n        # Pickup Timestamp\n        .withColumn(\"pickup_time\", F.col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n\n        # Enforce minimum start date\n        .filter(F.col(\"pickup_time\") >= start_date)\n\n        # Dropoff Timestamp\n        .withColumn(\"dropoff_time\", F.col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n\n        # Date Components\n        .withColumn(\"pickup_date\", F.to_date(F.col(\"pickup_time\")))\n        .withColumn(\"year\", F.year(F.col(\"pickup_time\")))\n        .withColumn(\"month\", F.month(F.col(\"pickup_time\")))\n        .withColumn(\"day\", F.dayofmonth(F.col(\"pickup_time\")))\n    )\n\nnormalized_raw = normalize_data(raw_df)\n\n# ---------------------------------------------------------------\n# Target Output Path (Sanitized for GitHub)\n# ---------------------------------------------------------------\ntarget_path = \"s3://<your-bucket>/bronze/nyc_taxi/\"\n\n# ---------------------------------------------------------------\n# Ingestion Metadata Columns\n# ---------------------------------------------------------------\nnow = datetime.now()\n\nfinal_bronze_df = (\n    normalized_raw\n    .withColumn(\"ingestion_date\", F.lit(now.strftime(\"%Y-%m-%d\")))\n    .withColumn(\"ingestion_year\", F.lit(now.year))\n    .withColumn(\"ingestion_month\", F.lit(now.month))\n    .withColumn(\"ingestion_day\", F.lit(now.day))\n)\n\n# ---------------------------------------------------------------\n# Write Partitioned Bronze Output\n# ---------------------------------------------------------------\nspecific_run_path = f\"{target_path}job_run_id={run_id}/\"\n\nfinal_bronze_df.write \\\n    .mode(\"append\") \\\n    .partitionBy(\"ingestion_year\", \"ingestion_month\", \"ingestion_day\") \\\n    .parquet(specific_run_path)\n\n# ---------------------------------------------------------------\n# Write Metadata for Downstream Jobs\n# ---------------------------------------------------------------\ns3_client = boto3.client(\"s3\")\n\nmetadata_bucket = \"<your-bucket>\"\nmetadata_key = f\"orchestration_metadata/bronze_to_silver_run_{run_id}.json\"\n\npayload = {\"silver_input_uri\": specific_run_path}\n\ns3_client.put_object(\n    Bucket=metadata_bucket,\n    Key=metadata_key,\n    Body=json.dumps(payload)\n)\n\nprint(f\"Metadata written to s3://{metadata_bucket}/{metadata_key}\")\n\n# ---------------------------------------------------------------\n# Final Commit\n# ---------------------------------------------------------------\njob.commit()\n"
}