import sys
from pyspark import SparkConf
from pyspark.context import SparkContext
from pyspark.sql import functions as F
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job

# ---------------------------------------------------------------
# Job Arguments
# ---------------------------------------------------------------
args = getResolvedOptions(sys.argv, ["JOB_NAME"])
job_name = args["JOB_NAME"]

# ---------------------------------------------------------------
# Spark / Iceberg Configuration
# ---------------------------------------------------------------
conf = SparkConf()
conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
conf.set("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog")
conf.set("spark.sql.catalog.glue_catalog.warehouse", "s3://<your-bucket>/gold")
conf.set("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
conf.set("spark.sql.catalog.glue_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")

sc = SparkContext(conf=conf)
glueContext = GlueContext(sc)
spark = glueContext.spark_session

job = Job(glueContext)
job.init(job_name, args)

# ---------------------------------------------------------------
# Source (Silver Iceberg Table)
# ---------------------------------------------------------------
silver_full = "glue_catalog.silver_iceberg.nyc_taxi"
silver_df = spark.table(silver_full)

# ---------------------------------------------------------------
# GOLD AGGREGATION â€” Daily Metrics
# ---------------------------------------------------------------
gold_updates = (
    silver_df
    .groupBy(
        "pickup_date",
        "year",
        "month",
        "day",
        "is_weekend"
    )
    .agg(
        F.count("*").alias("trip_count"),
        F.sum("total_amount").alias("total_revenue"),
        F.sum("fare_amount").alias("total_fare"),
        F.sum("tip_amount").alias("total_tip_amount"),
        F.round(F.avg("trip_distance"), 2).alias("avg_trip_distance"),
        F.round(F.avg("passenger_count"), 2).alias("avg_passenger_count"),
        F.round(
            F.avg(F.col("tip_amount") / F.col("fare_amount")),
            4
        ).alias("avg_tip_pct"),
        F.round(F.sum("charges"), 2).alias("total_extra_charges")
    )
)

gold_updates.createOrReplaceTempView("incoming_gold")

# ---------------------------------------------------------------
# GOLD TABLE CONFIG
# ---------------------------------------------------------------
gold_db = "gold_iceberg"
gold_table = "daily_taxi_metrics"
gold_full = f"glue_catalog.{gold_db}.{gold_table}"

# ---------------------------------------------------------------
# Create Gold Table if Missing
# ---------------------------------------------------------------
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {gold_full}
    USING iceberg
    PARTITIONED BY (year, month, day)
    AS SELECT * FROM incoming_gold WHERE 1 = 0
""")

# ---------------------------------------------------------------
# Generate MERGE Column Lists
# ---------------------------------------------------------------
cols = gold_updates.columns

update_set = ", ".join([f"target.{c} = source.{c}" for c in cols])
insert_cols = ", ".join(cols)
insert_vals = ", ".join([f"source.{c}" for c in cols])

# ---------------------------------------------------------------
# MERGE INTO (Incremental Upsert)
# ---------------------------------------------------------------
spark.sql(f"""
    MERGE INTO {gold_full} AS target
    USING incoming_gold AS source
    ON target.pickup_date = source.pickup_date

    WHEN MATCHED THEN
        UPDATE SET {update_set}

    WHEN NOT MATCHED THEN
        INSERT ({insert_cols})
        VALUES ({insert_vals})
""")

print(f"Gold MERGE completed: {gold_full}")

job.commit()
